#include <atomic>
#include <cuda.h>
#include <cuda/atomic>
#include <cuda_runtime.h>
#include <nvrtc.h>
#define CUDA_CODE 1

class atomic_bool_gpu
{
  private:
    cuda::atomic<int, cuda::thread_scope_device> value;

  public:
    __host__ __device__ atomic_bool_gpu() : value(0) {}

    __host__ __device__ atomic_bool_gpu(bool initial_value)
        : value(initial_value ? 1 : 0)
    {
    }

    __host__ __device__ void store(bool new_value)
    {
        value.store(new_value ? 1 : 0, cuda::memory_order_relaxed);
    }

    __host__ __device__ bool load() const
    {
        return value.load(cuda::memory_order_relaxed) != 0;
    }

    __host__ __device__ bool exchange(bool new_value)
    {
        return value.exchange(new_value ? 1 : 0, cuda::memory_order_relaxed) !=
               0;
    }

    __host__ __device__ bool compare_exchange(bool expected, bool desired)
    {
        int expected_int = expected ? 1 : 0;
        return value.compare_exchange_strong(
            expected_int,
            desired ? 1 : 0,
            cuda::memory_order_relaxed
        );
    }

    // Explicit conversion operator to bool
    __host__ __device__ explicit operator bool() const { return load(); }
};

using shared_atomic_bool = atomic_bool_gpu;
using atomic_bool        = atomic_bool_gpu;

#define DEV    __device__
#define KERNEL __global__
#define DUAL   __host__ __device__
#define STATIC __host__ __device__ inline
#define EXTERN extern __shared__

#if FLOAT_PRECISION
using atomic_cast = int;
#define __int_as_real __int_as_float
#define __real_as_int __float_as_int
#else
using atomic_cast = long long;
#define __int_as_real __longlong_as_double
#define __real_as_int __double_as_longlong
#endif

using sig_bool = volatile bool;
#define SINGLE(kernel_name, ...) kernel_name<<<1, 1>>>(__VA_ARGS__);

#define CALL(kernel_name, gridsize, blocksize, ...)                            \
    kernel_name<<<(gridsize), (blocksize)>>>(__VA_ARGS__);

namespace global {
#if SHARED_MEMORY
#define SHARED __device__
    // shorthand flag for using gpu shared memory
    constexpr bool on_sm = true;
#else
#define SHARED __device__ const
    // shorthand flag for using gpu shared memory
    constexpr bool on_sm = false;
#endif
    constexpr Platform BuildPlatform = Platform::GPU;
}   // namespace global

namespace global {
    constexpr int WARP_SIZE = 32;
}   // namespace global

template <typename T>
constexpr auto devMalloc(T** devPtr, size_t size)
{
    return cudaMalloc(devPtr, size);
}

template <typename T>
constexpr auto devMallocManaged(T** devPtr, size_t size)
{
    return cudaMallocManaged(devPtr, size);
}

inline auto devEventCreate(cudaEvent_t* stamp)
{
    return cudaEventCreate(stamp);
};

inline auto devEventRecord(cudaEvent_t stamp)
{
    return cudaEventRecord(stamp);
};

inline auto devMemcpyFromSymbol(void* dst, const void* symbol, size_t count)
{
    return cudaMemcpyFromSymbol(dst, symbol, count);
};

inline auto devMallocHost(void** ptr, size_t size)
{
    return cudaMallocHost(ptr, size);
};

inline auto devLaunchKernel(
    const void* func,
    dim3 grid,
    dim3 block,
    void** args,
    size_t sharedMem,
    cudaStream_t stream
)
{
    return cudaLaunchKernel(func, grid, block, args, sharedMem, stream);
};

constexpr auto devMemcpy               = cudaMemcpy;
constexpr auto devFree                 = cudaFree;
constexpr auto devMemset               = cudaMemset;
constexpr auto devDeviceSynchronize    = cudaDeviceSynchronize;
constexpr auto devMemcpyHostToDevice   = cudaMemcpyHostToDevice;
constexpr auto devMemcpyDeviceToDevice = cudaMemcpyDeviceToDevice;
constexpr auto devMemcpyDeviceToHost   = cudaMemcpyDeviceToHost;
constexpr auto devGetErrorString       = cudaGetErrorString;
constexpr auto devEventDestroy         = cudaEventDestroy;
constexpr auto devEventSynchronize     = cudaEventSynchronize;
constexpr auto devEventElapsedTime     = cudaEventElapsedTime;
constexpr auto devGetDeviceProperties  = cudaGetDeviceProperties;
constexpr auto devGetDeviceCount       = cudaGetDeviceCount;
constexpr auto devSetDevice            = cudaSetDevice;
constexpr auto devStreamCreate         = cudaStreamCreate;
constexpr auto devStreamDestroy        = cudaStreamDestroy;
constexpr auto devStreamSynchronize    = cudaStreamSynchronize;
constexpr auto devStreamWaitEvent      = cudaStreamWaitEvent;
constexpr auto devStreamQuery          = cudaStreamQuery;
constexpr auto devEnablePeerAccess     = cudaDeviceEnablePeerAccess;
constexpr auto devGetDevice            = cudaGetDevice;
constexpr auto devMemcpyPeerAsync      = cudaMemcpyPeerAsync;
constexpr auto devMemcpyAsync          = cudaMemcpyAsync;
constexpr auto devHostRegister         = cudaHostRegister;
constexpr auto devHostUnregister       = cudaHostUnregister;
// constexpr auto devLaunchKernel         = cudaLaunchKernel;

using devProp_t       = cudaDeviceProp;
using devError_t      = cudaError_t;
using devEvent_t      = cudaEvent_t;
using simbiStream_t   = cudaStream_t;
using simbiMemcpyKind = cudaMemcpyKind;
// tools for JIT compilation
constexpr auto devCompileProgram    = nvrtcCompileProgram;
constexpr auto devCreateProgram     = nvrtcCreateProgram;
constexpr auto devGetIRSize         = nvrtcGetPTXSize;
constexpr auto devGetIR             = nvrtcGetPTX;
constexpr auto devGetProgramLogSize = nvrtcGetProgramLogSize;
constexpr auto devGetProgramLog     = nvrtcGetProgramLog;
constexpr auto devDestroyProgram    = nvrtcDestroyProgram;
constexpr auto devLoadModule        = cuModuleLoadData;
constexpr auto devUnloadModule      = cuModuleUnload;
constexpr auto devGetFunction       = cuModuleGetFunction;

using devProgram_t  = nvrtcProgram;
using devContext_t  = CUcontext;
using devModule_t   = CUmodule;
using devDevice_t   = CUdevice;
using devFunction_t = CUfunction;
