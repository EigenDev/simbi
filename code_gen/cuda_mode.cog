#include <atomic>
#include <cuda.h>
#include <cuda/atomic>
#include <cuda_profiler_api.h>
#include <cuda_runtime.h>
#define CUDA_CODE 1

template <typename ManagedType>
class atomic_bool_gpu : public ManagedType
{
  private:
    cuda::atomic<bool, cuda::thread_scope_device> value;

  public:
    __host__ __device__ atomic_bool_gpu() : value(false) {}

    __host__ __device__ atomic_bool_gpu(bool initial_value)
        : value(initial_value)
    {
    }

    __host__ __device__ void store(bool new_value)
    {
        value.store(new_value, cuda::memory_order_relaxed);
    }

    __host__ __device__ bool load() const
    {
        return value.load(cuda::memory_order_relaxed);
    }

    __host__ __device__ bool exchange(bool new_value)
    {
        return value.exchange(new_value, cuda::memory_order_relaxed);
    }

    __host__ __device__ bool compare_exchange(bool expected, bool desired)
    {
        bool expected_bool = expected;
        return value.compare_exchange_strong(
            expected_bool,
            desired,
            cuda::memory_order_relaxed
        );
    }

    // Explicit conversion operator to bool
    __host__ __device__ explicit operator bool() const { return load(); }
};

template <typename T>
using shared_atomic_bool = atomic_bool_gpu<T>;

#define DEV        __device__
#define KERNEL     __global__
#define DUAL       __host__ __device__
#define STATIC     __host__ __device__ inline
#define EXTERN     extern __shared_
#define STATIC_VAR __device__ volatile
#define SHARED     __shared__

#if FLOAT_PRECISION
using atomic_cast = int;
#define __int_as_real __int_as_float
#define __real_as_int __float_as_int
#else
using atomic_cast = long long;
#define __int_as_real __longlong_as_double
#define __real_as_int __double_as_longlong
#endif

using sig_bool = volatile bool;
#define SINGLE(kernel_name, ...) kernel_name<<<1, 1>>>(__VA_ARGS__);

#define CALL(kernel_name, gridsize, blocksize, ...)                            \
    kernel_name<<<(gridsize), (blocksize)>>>(__VA_ARGS__);

namespace global {
#if SHARED_MEMORY
    // shorthand flag for using gpu shared memory
    constexpr bool on_sm = true;
#else
    // shorthand flag for using gpu shared memory
    constexpr bool on_sm = false;
#endif
    constexpr Platform BuildPlatform = Platform::GPU;
}   // namespace global

namespace global {
    constexpr size_type WARP_SIZE = 32;
}   // namespace global

template <typename T>
constexpr auto devMalloc(T** devPtr, size_t size)
{
    return cudaMalloc(devPtr, size);
}

template <typename T>
constexpr auto devMallocManaged(T** devPtr, size_t size)
{
    return cudaMallocManaged(devPtr, size);
}

inline auto devEventCreate(cudaEvent_t* stamp)
{
    return cudaEventCreate(stamp);
};

inline auto devEventRecord(cudaEvent_t stamp)
{
    return cudaEventRecord(stamp);
};

inline auto devMemcpyFromSymbol(void* dst, const void* symbol, size_t count)
{
    return cudaMemcpyFromSymbol(dst, symbol, count);
};

inline auto devMallocHost(void** ptr, size_t size)
{
    return cudaMallocHost(ptr, size);
};

inline auto devLaunchKernel(
    const void* func,
    dim3 grid,
    dim3 block,
    void** args,
    size_t sharedMem,
    cudaStream_t stream
)
{
    return cudaLaunchKernel(func, grid, block, args, sharedMem, stream);
};

__device__ __forceinline__ real devAtomicMinReal(real* addr, real value)
{
    real old =
        __int_as_real(atomicMin((atomic_cast*) addr, __real_as_int(value)));
    return old;
}

__device__ __forceinline__ auto devAtomicAddInt(int* addr, int value)
{
    return atomicAdd(addr, value);
}

__device__ __forceinline__ real devAtomicAddReal(real* addr, real value)
{
#if FLOAT_PRECISION
    return atomicAdd(addr, value);   // Built-in float atomic
#else
    unsigned long long int* address_as_ull = (unsigned long long int*) addr;
    unsigned long long int old             = *address_as_ull;
    unsigned long long int assumed;
    do {
        assumed = old;
        old     = atomicCAS(
            address_as_ull,
            assumed,
            __double_as_longlong(value + __longlong_as_double(assumed))
        );
    } while (assumed != old);
    return __longlong_as_double(old);
#endif
}

template <typename T>
__device__ __forceinline__ T devAtomicMin(T* addr, T value)
{
    return atomicMin(addr, value);
}

inline auto devPrefetchAsync(const void* dec_ptr, size_t size, int dst_device, cudaStream_t stream = 0)
{
    return cudaMemPrefetchAsync(dec_ptr, size, dst_device, stream);
}

constexpr auto devMemcpy               = cudaMemcpy;
constexpr auto devFree                 = cudaFree;
constexpr auto devMemset               = cudaMemset;
constexpr auto devDeviceSynchronize    = cudaDeviceSynchronize;
constexpr auto devMemcpyHostToDevice   = cudaMemcpyHostToDevice;
constexpr auto devMemcpyDeviceToDevice = cudaMemcpyDeviceToDevice;
constexpr auto devMemcpyDeviceToHost   = cudaMemcpyDeviceToHost;
constexpr auto devGetErrorString       = cudaGetErrorString;
constexpr auto devEventDestroy         = cudaEventDestroy;
constexpr auto devEventSynchronize     = cudaEventSynchronize;
constexpr auto devEventElapsedTime     = cudaEventElapsedTime;
constexpr auto devGetDeviceProperties  = cudaGetDeviceProperties;
constexpr auto devGetDeviceCount       = cudaGetDeviceCount;
constexpr auto devSetDevice            = cudaSetDevice;
constexpr auto devStreamCreate         = cudaStreamCreate;
constexpr auto devStreamDestroy        = cudaStreamDestroy;
constexpr auto devStreamSynchronize    = cudaStreamSynchronize;
constexpr auto devStreamWaitEvent      = cudaStreamWaitEvent;
constexpr auto devStreamQuery          = cudaStreamQuery;
constexpr auto devEnablePeerAccess     = cudaDeviceEnablePeerAccess;
constexpr auto devGetDevice            = cudaGetDevice;
constexpr auto devMemcpyPeerAsync      = cudaMemcpyPeerAsync;
constexpr auto devMemcpyAsync          = cudaMemcpyAsync;
constexpr auto devHostRegister         = cudaHostRegister;
constexpr auto devHostUnregister       = cudaHostUnregister;

using devProp_t       = cudaDeviceProp;
using devError_t      = cudaError_t;
using devEvent_t      = cudaEvent_t;
using simbiStream_t   = cudaStream_t;
using simbiMemcpyKind = cudaMemcpyKind;

using devContext_t  = CUcontext;
using devModule_t   = CUmodule;
using devDevice_t   = CUdevice;
using devFunction_t = CUfunction;
