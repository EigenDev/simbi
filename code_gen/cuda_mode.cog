#include <cuda_runtime.h>
#include <cuda/atomic>
#include <atomic>
#define CUDA_CODE 1



class atomic_bool_gpu {
private:
    cuda::atomic<int, cuda::thread_scope_device> value;

public:
    __host__ __device__ atomic_bool_gpu() : value(0) {}
    __host__ __device__ atomic_bool_gpu(bool initial_value) : value(initial_value ? 1 : 0) {}

    __host__ __device__ void store(bool new_value) {
        value.store(new_value ? 1 : 0, cuda::memory_order_relaxed);
    }

    __host__ __device__ bool load() const {
        return value.load(cuda::memory_order_relaxed) != 0;
    }

    __host__ __device__ bool exchange(bool new_value) {
        return value.exchange(new_value ? 1 : 0, cuda::memory_order_relaxed) != 0;
    }

    __host__ __device__ bool compare_exchange(bool expected, bool desired) {
        int expected_int = expected ? 1 : 0;
        return value.compare_exchange_strong(expected_int, desired ? 1 : 0, cuda::memory_order_relaxed);
    }

    // Explicit conversion operator to bool
    __host__ __device__ explicit operator bool() const {
        return load();
    }
};

using atomic_bool_shared = atomic_bool_gpu;
using atomic_bool = atomic_bool_gpu;

#define DEV    __device__
#define KERNEL __global__
#define DUAL   __host__ __device__
#define STATIC __host__ __device__ inline
#define EXTERN extern __shared__

#if FLOAT_PRECISION
using atomic_cast = int;
#define __int_as_real __int_as_float
#define __real_as_int __float_as_int
#else
using atomic_cast = long long;
#define __int_as_real __longlong_as_double
#define __real_as_int __double_as_longlong
#endif

using sig_bool = volatile bool;
#define SINGLE(kernel_name, ...) kernel_name<<<1, 1>>>(__VA_ARGS__);

#define CALL(kernel_name, gridsize, blocksize, ...)                            \
    kernel_name<<<(gridsize), (blocksize)>>>(__VA_ARGS__);

namespace global {
#if SHARED_MEMORY
#define SHARED __device__
    // shorthand flag for using gpu shared memory
    constexpr bool on_sm = true;
#else
#define SHARED __device__ const
    // shorthand flag for using gpu shared memory
    constexpr bool on_sm = false;
#endif
    constexpr Platform BuildPlatform = Platform::GPU;
}   // namespace global

namespace global {
    constexpr int WARP_SIZE = 32;
}   // namespace global

template <typename T>
constexpr auto devMalloc(T** devPtr, size_t size)
{
    return cudaMalloc(devPtr, size);
}

template <typename T>
constexpr auto devMallocManaged(T** devPtr, size_t size)
{
    return cudaMallocManaged(devPtr, size);
}

inline auto devEventCreate(cudaEvent_t* stamp)
{
    return cudaEventCreate(stamp);
};

inline auto devEventRecord(cudaEvent_t stamp)
{
    return cudaEventRecord(stamp);
};

inline auto devMemcpyFromSymbol(void* dst, const void* symbol, size_t count)
{
    return cudaMemcpyFromSymbol(dst, symbol, count);
};

constexpr auto devMemcpy               = cudaMemcpy;
constexpr auto devFree                 = cudaFree;
constexpr auto devMemset               = cudaMemset;
constexpr auto devDeviceSynchronize    = cudaDeviceSynchronize;
constexpr auto devMemcpyHostToDevice   = cudaMemcpyHostToDevice;
constexpr auto devMemcpyDeviceToDevice = cudaMemcpyDeviceToDevice;
constexpr auto devMemcpyDeviceToHost   = cudaMemcpyDeviceToHost;
constexpr auto devGetErrorString       = cudaGetErrorString;
constexpr auto devEventDestroy         = cudaEventDestroy;
constexpr auto devEventSynchronize     = cudaEventSynchronize;
constexpr auto devEventElapsedTime     = cudaEventElapsedTime;
constexpr auto devGetDeviceProperties  = cudaGetDeviceProperties;
constexpr auto devGetDeviceCount       = cudaGetDeviceCount;
using devProp_t                        = cudaDeviceProp;
using devError_t                       = cudaError_t;
using devEvent_t                       = cudaEvent_t;
using simbiStream_t                    = cudaStream_t;